##################
# Trainer settings
##################

MODEL	MeetingNet_Transformer_ernie_cpu
TASK	HMNet
CRITERION MLECriterion

SEED    1033
RESUME

MAX_NUM_EPOCHS	60
EVAL_PER_UPDATE_NUM	10
UPDATES_PER_EPOCH 20

# The actuall learning rate will be multiplied with the number of GPUs
OPTIMIZER	RAdam
NO_AUTO_LR_SCALING
START_LEARNING_RATE	1e-3
LR_SCHEDULER	LnrWrmpInvSqRtDcyScheduler
WARMUP_STEPS	16000
WARMUP_INIT_LR  1e-4
WARMUP_END_LR 1e-3

# The actuall start learning rate equals START_LEARNING_RATE * GRADIENT_ACCUMULATE_STEP
# Model will be updated after every MINI_BATCH * GRADIENT_ACCUMULATE_STEP samples
GRADIENT_ACCUMULATE_STEP	10

GRAD_CLIPPING    2

##################
# Task settings
##################

# This is the relative path to the directory where this conf file locates
# not a good idea to put data with code
# Are we able to provide a list of dir paths in TRAIN_FILE?
USE_REL_DATA_PATH
TRAIN_FILE	../ExampleRawData/meeting_summarization//ICSI_proprec/train_icsi.json
DEV_FILE	../ExampleRawData/meeting_summarization/ICSI_proprec/valid_icsi.json
TEST_FILE ../ExampleRawData/meeting_summarization/ICSI_proprec/test_icsi.json
GPT2CONFIG_PATH /cluster/home/qimengnan/HMNet/ExampleRawData/meeting_summarization/gpt2_config.json
ERNIECONFIG_PATH /cluster/home/qimengnan/HMNet/ExampleRawData/meeting_summarization/ernie_config.json
ROLE_DICT_FILE  ../ExampleRawData/meeting_summarization/role_dict_ms.json

MINI_BATCH	1
MAX_PADDING_RATIO	1
BATCH_READ_AHEAD	10
DOC_SHUFFLE_BUF_SIZE	10
SAMPLE_SHUFFLE_BUFFER_SIZE	10
BATCH_SHUFFLE_BUFFER_SIZE	10

#MAX_TRANSCRIPT_WORD 改变这个就能调整batch最大允许长度
MAX_TRANSCRIPT_WORD 8300
MAX_SENT_LEN 30
MAX_SENT_NUM 300

##################
# Model settings
##################

DROPOUT	0
VOCAB_DIM	512
ROLE_SIZE   64
ROLE_DIM	100
POS_DIM 16
ENT_DIM 16

#USE_ROLE
USE_POSENT
#USE_ENTMASK

USE_BOS_TOKEN
USE_EOS_TOKEN

TRANSFORMER_EMBED_DROPOUT	0
TRANSFORMER_RESIDUAL_DROPOUT	0
TRANSFORMER_ATTENTION_DROPOUT	0
WORLDLEVEL_LAYER	6
TURNLEVEL_LAYER		6
DECODER_LAYER		6
TRANSFORMER_HEAD	8
TRANSFORMER_POS_DISCOUNT	80
TYPE_POOLING	MAX

PRE_TOKENIZER	TransfoXLTokenizer
PRE_TOKENIZER_PATH  ../ExampleInitModel/transfo-xl-wt103  
PYLEARN_MODEL	../ExampleInitModel/ICSI_Finetuned

##################
# Tokenizer settings
##################

EXTRA_IDS	1000

##################
# Decoding settings
##################

BEAM_WIDTH      6
MAX_GEN_LENGTH  512
MIN_GEN_LENGTH  280
EVAL_TOKENIZED
EVAL_LOWERCASE
DO_SAMPLE	False
#TEMPERATURE	0.7
#REPETITION_PENALTY	1.2
#LENGTH_PENALTY	1.1
NO_REPEAT_NGRAM_SIZE 3