##################
# Trainer settings
##################

MODEL	MeetingNet_Transformer_ernie_cpu
TASK	HMNet
CRITERION MLECriterion

SEED    1033
RESUME

MAX_NUM_EPOCHS	60
SAVE_PER_UPDATE_NUM	100
UPDATES_PER_EPOCH 2000

# The actuall learning rate will be multiplied with the number of GPUs
OPTIMIZER	RAdam
NO_AUTO_LR_SCALING
START_LEARNING_RATE	1e-3
LR_SCHEDULER	LnrWrmpInvSqRtDcyScheduler
WARMUP_STEPS	16000
WARMUP_INIT_LR 1e-4
WARMUP_END_LR	1e-3

# The actuall start learning rate equals START_LEARNING_RATE * GRADIENT_ACCUMULATE_STEP
# Model will be updated after every MINI_BATCH * GRADIENT_ACCUMULATE_STEP samples
GRADIENT_ACCUMULATE_STEP	20

GRAD_CLIPPING    2

##################
# Task settings
##################

# This is the relative path to the directory where this conf file locates
# not a good idea to put data with code
# Are we able to provide a list of dir paths in TRAIN_FILE?
USE_REL_DATA_PATH
TRAIN_FILE	../ExampleRawData/meeting_summarization/AMI_clean/train_ami.json
DEV_FILE	../ExampleRawData/meeting_summarization/AMI_clean/valid_ami.json
TEST_FILE ../ExampleRawData/meeting_summarization/AMI_clean/test_ami.json
GPT2CONFIG_PATH /cluster/home/qimengnan/HMNet/ExampleRawData/meeting_summarization/gpt2_config.json
ERNIECONFIG_PATH /cluster/home/qimengnan/HMNet/ExampleRawData/meeting_summarization/ernie_config.json
ROLE_DICT_FILE  ../ExampleRawData/meeting_summarization/role_dict_ms.json

MINI_BATCH	1
MAX_PADDING_RATIO	1
BATCH_READ_AHEAD	10
DOC_SHUFFLE_BUF_SIZE	10
SAMPLE_SHUFFLE_BUFFER_SIZE	10
BATCH_SHUFFLE_BUFFER_SIZE	10

#MAX_TRANSCRIPT_WORD 改变这个就能调整batch最大允许长度
MAX_TRANSCRIPT_WORD 8300
MAX_SENT_LEN 30
MAX_SENT_NUM 300

##################
# Model settings
##################

DROPOUT	0.1
VOCAB_DIM	512
ROLE_SIZE   64
ROLE_DIM	100
POS_DIM 16
ENT_DIM 16

#USE_ROLE
USE_POSENT
#USE_ENTMASK

USE_BOS_TOKEN
USE_EOS_TOKEN

TRANSFORMER_EMBED_DROPOUT	0.1
TRANSFORMER_RESIDUAL_DROPOUT	0.1
TRANSFORMER_ATTENTION_DROPOUT	0.1
WORLDLEVEL_LAYER	6
TURNLEVEL_LAYER		6
DECODER_LAYER		6
TRANSFORMER_HEAD	8
TRANSFORMER_POS_DISCOUNT	80
TYPE_POOLING	MAX	

PRE_TOKENIZER	TransfoXLTokenizer
PRE_TOKENIZER_PATH  ../ExampleInitModel/transfo-xl-wt103  
#PYLEARN_MODEL conf_multitask_onemask_textrank_conf~/run_2/15200
#PYLEARN_MODEL conf_multitask_rolemask_conf~/run_4/10800
#PYLEARN_MODEL	conf_hmnet_pretrain_ernie_mediasum_conf~/run_2/507200
#PYLEARN_MODEL	conf_hmnet_pretrain_ernie_textrank_conf~/run_1/102400
PYLEARN_MODEL	conf_hmnet_pretrain_ernie_cnndm_conf~/run_2/1020800

##################
# Tokenizer settings
##################

EXTRA_IDS	1000

##################
# Decoding settings
##################

BEAM_WIDTH      6
MAX_GEN_LENGTH  512
MIN_GEN_LENGTH  280
EVAL_TOKENIZED
EVAL_LOWERCASE
NO_REPEAT_NGRAM_SIZE 3